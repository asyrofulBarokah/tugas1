{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nim : 160411100070 Nama : Asyroful Barokah Mata Kuliah : Penambangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom. Tools dan Library yang harus di instal Tools : Python 3.7 Library : BeatifulSoap4 : untuk crawling request : untuk crawling pandas : untuk memudahkan memrosesan dan ekspoitasi data network : menampilkan graph dari hasil codingan yang dijalankan matplotlib.pyplot : membuat grafik yang rapi dalam cara yang mudah target web : http://transtv.co.id/","title":"Pengantar"},{"location":"#tools-dan-library-yang-harus-di-instal","text":"Tools : Python 3.7 Library : BeatifulSoap4 : untuk crawling request : untuk crawling pandas : untuk memudahkan memrosesan dan ekspoitasi data network : menampilkan graph dari hasil codingan yang dijalankan matplotlib.pyplot : membuat grafik yang rapi dalam cara yang mudah target web : http://transtv.co.id/","title":"Tools dan Library yang harus di instal"},{"location":"Evaluasi/","text":"Evaluasi Crawl data dari website https://www.jualmotorbekas.com/ mendapatkan 264 artikel dan banyak kata yang telah diproses menjadi 173 kata. serta dari metode yang telah dipakai pada code diatas dapat mengambil kata dari sekian banyak artikel/dokumen website yang baku sesuai KBBI dan mengelompokkannya berdasarkan clustering yang diinginkan Hasil Running mengambil data dan menghilangkan kata tidak penting(kata imbuhan) hasil seleksi kata dengan KBI hasil seleksi fitur hasil k-means dan silhouete","title":"Evaluasi"},{"location":"Evaluasi/#evaluasi","text":"Crawl data dari website https://www.jualmotorbekas.com/ mendapatkan 264 artikel dan banyak kata yang telah diproses menjadi 173 kata. serta dari metode yang telah dipakai pada code diatas dapat mengambil kata dari sekian banyak artikel/dokumen website yang baku sesuai KBBI dan mengelompokkannya berdasarkan clustering yang diinginkan","title":"Evaluasi"},{"location":"Evaluasi/#hasil-running","text":"mengambil data dan menghilangkan kata tidak penting(kata imbuhan) hasil seleksi kata dengan KBI hasil seleksi fitur hasil k-means dan silhouete","title":"Hasil Running"},{"location":"Evaluasi2/","text":"Evaluasi hasil program link yang dicrawl : pagerank : graph","title":"Hasil Running"},{"location":"Evaluasi2/#evaluasi","text":"","title":"Evaluasi"},{"location":"Evaluasi2/#hasil-program","text":"link yang dicrawl : pagerank : graph","title":"hasil program"},{"location":"Instal/","text":"Nim : 160411100070 Nama : Asyroful Barokah Mata Kuliah : Penambangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom. Pengertian Web Crawler web crawler merupakan suatu alat atau program yang digunakan search engine untuk meng index atau menjelajahi seluruh web yang ada di internet, digunakan untuk membuat salinan sebagian atau keseluruhan halaman web yang telah dikunjunginya agar dapat diproses lebih lanjut oleh system pengindeksan Tools dan Library yang harus di instal Tools : Python 3.6 Library : BeatifulSoap4 : untuk crawling request : untuk crawling sqlite3 : import ke database csv : import ke csv Sastrawi : preprocessing text numpy : untuk seleksi fitur dan clustering scipy : untuk seleksi fitur dan clustering Scikit learn : untuk seleksi fitur dan clustering target web : https://www.jualmotorbekas.com/","title":"Pengantar"},{"location":"Instal/#pengertian-web-crawler","text":"web crawler merupakan suatu alat atau program yang digunakan search engine untuk meng index atau menjelajahi seluruh web yang ada di internet, digunakan untuk membuat salinan sebagian atau keseluruhan halaman web yang telah dikunjunginya agar dapat diproses lebih lanjut oleh system pengindeksan","title":"Pengertian Web Crawler"},{"location":"Instal/#tools-dan-library-yang-harus-di-instal","text":"Tools : Python 3.6 Library : BeatifulSoap4 : untuk crawling request : untuk crawling sqlite3 : import ke database csv : import ke csv Sastrawi : preprocessing text numpy : untuk seleksi fitur dan clustering scipy : untuk seleksi fitur dan clustering Scikit learn : untuk seleksi fitur dan clustering target web : https://www.jualmotorbekas.com/","title":"Tools dan Library yang harus di instal"},{"location":"clustering/","text":"Clustering for i in range(2, len(fiturBaru)-1): kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru); print( kmeans = ,kmeans.labels_) classnya = kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print ( silhouette = , s_avg) print( silhouette untuk ,i, cluster : ,s_avg) print(kmeans.labels_) Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Code di atas adalah code untuk melakukan clustering dengan menggunakan metode kmeans. Pada contoh ini cluster dibagi menjadi 5. Banyak cluster bisa diubah sesuai kebutuhan. Silhouette dilakukan untuk mengetahui seberapa dekat relasi antara objek dalam sebuah cluster dan seberapa jauh sebuah cluster terpisah dengan cluster lain, gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain.","title":"Clustering"},{"location":"clustering/#clustering","text":"for i in range(2, len(fiturBaru)-1): kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru); print( kmeans = ,kmeans.labels_) classnya = kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print ( silhouette = , s_avg) print( silhouette untuk ,i, cluster : ,s_avg) print(kmeans.labels_) Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Code di atas adalah code untuk melakukan clustering dengan menggunakan metode kmeans. Pada contoh ini cluster dibagi menjadi 5. Banyak cluster bisa diubah sesuai kebutuhan. Silhouette dilakukan untuk mengetahui seberapa dekat relasi antara objek dalam sebuah cluster dan seberapa jauh sebuah cluster terpisah dengan cluster lain, gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain.","title":"Clustering"},{"location":"crawling/","text":"Crawling src = https://www.jualmotorbekas.com/ page = requests.get(src) soup = BeautifulSoup(page.content, html.parser ) bagan = soup.findAll(class_= item-header ) koneksi = sqlite3.connect( DB_motor.db ) koneksi.execute( CREATE TABLE if not exists jualMotorBekas (judul TEXT NOT NULL, isi TEXT NOT NULL); ) for i in range(len(bagan)): link = bagan[i].find( a )[ href ] page = requests.get(link) soup = BeautifulSoup(page.content, html.parser ) judul = soup.find(class_= single-listing ).getText() isi = soup.find(class_= single-main ) paragraf = isi.findAll( p ) p = for s in paragraf: p+=s.getText() + \u200b``` koneksi.execute( INSERT INTO jualMotorBekas values (?,?) , (judul, p)); \u200b``` koneksi.commit() tampil = koneksi.execute( SELECT * FROM jualMotorBekas ) with open ( data_crawler.csv , newline= , mode= w )as employee_file : employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) \u200b``` for i in tampil: employee_writer.writerow(i) \u200b``` tampil = koneksi.execute( SELECT * FROM jualMotorBekas ) isi = [] for row in tampil: isi.append(row[1]) penjelasan code crawling data : src = https://www.jualmotorbekas.com/ page = requests.get(src) soup = BeautifulSoup(page.content, html.parser ) bagan = soup.findAll(class_= item-header ) Code diatas adalah code yang menyambungkan dengan website www.jualmotorbekas.com class yang akan di ambil datanya adalah class 'item-header'. saya mengambil contoh web dari jual motor bekas. koneksi = sqlite3.connect( DB_motor.db ) koneksi.execute( CREATE TABLE if not exists jualMotorBekas (judul TEXT NOT NULL, isi TEXT NOT NULL); ) Code diatas merupakan code yang berfungsi untuk membuat database yang bernama 'DB_motor.db' dan dibuat juga table yang bernama 'jualMotorBekas' dengan 2 kolom yang bernama 'judul' dan 'isi'. for i in range(len(bagan)): link = bagan[i].find( a )[ href ] page = requests.get(link) soup = BeautifulSoup(page.content, html.parser ) judul = soup.find(class_= single-listing ).getText() isi = soup.find(class_= single-main ) paragraf = isi.findAll( p ) p = for s in paragraf: p+=s.getText() + koneksi.execute( INSERT INTO jualMotorBekas values (?,?) , (judul, p)); Code diatas memiliki fungsi jika \"bagan = soup.findAll(class_='item-header')\" diklik akan menuju ke alamat setiap motor bekas yang dijual. mengambil data dari class 'single-listing' yang berisi judul/header motor yang dijual. dan akan mengambil data dari class 'single-main' yang berisi isi/deskripsi dari motor yang dijual. paragraf berfungsi untuk mengambil data sesuai dengan bentuk paragraf. dan akan diinsertkan/dimasukkan data yang di ambil ke dalam table jualMotorBekas berdasarkan kolom (judul,p). koneksi.commit() tampil = koneksi.execute( SELECT * FROM jualMotorBekas ) with open ( data_crawler.csv , newline= , mode= w )as employee_file : employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) \u200b``` for i in tampil: employee_writer.writerow(i) \u200b``` dari data yang telah diambil akan dimasukkan/ditulis kedalam bentuk csv dengan nama data_crawler.csv.","title":"Crawl"},{"location":"crawling/#crawling","text":"src = https://www.jualmotorbekas.com/ page = requests.get(src) soup = BeautifulSoup(page.content, html.parser ) bagan = soup.findAll(class_= item-header ) koneksi = sqlite3.connect( DB_motor.db ) koneksi.execute( CREATE TABLE if not exists jualMotorBekas (judul TEXT NOT NULL, isi TEXT NOT NULL); ) for i in range(len(bagan)): link = bagan[i].find( a )[ href ] page = requests.get(link) soup = BeautifulSoup(page.content, html.parser ) judul = soup.find(class_= single-listing ).getText() isi = soup.find(class_= single-main ) paragraf = isi.findAll( p ) p = for s in paragraf: p+=s.getText() + \u200b``` koneksi.execute( INSERT INTO jualMotorBekas values (?,?) , (judul, p)); \u200b``` koneksi.commit() tampil = koneksi.execute( SELECT * FROM jualMotorBekas ) with open ( data_crawler.csv , newline= , mode= w )as employee_file : employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) \u200b``` for i in tampil: employee_writer.writerow(i) \u200b``` tampil = koneksi.execute( SELECT * FROM jualMotorBekas ) isi = [] for row in tampil: isi.append(row[1])","title":"Crawling"},{"location":"crawling/#penjelasan-code-crawling-data","text":"src = https://www.jualmotorbekas.com/ page = requests.get(src) soup = BeautifulSoup(page.content, html.parser ) bagan = soup.findAll(class_= item-header ) Code diatas adalah code yang menyambungkan dengan website www.jualmotorbekas.com class yang akan di ambil datanya adalah class 'item-header'. saya mengambil contoh web dari jual motor bekas. koneksi = sqlite3.connect( DB_motor.db ) koneksi.execute( CREATE TABLE if not exists jualMotorBekas (judul TEXT NOT NULL, isi TEXT NOT NULL); ) Code diatas merupakan code yang berfungsi untuk membuat database yang bernama 'DB_motor.db' dan dibuat juga table yang bernama 'jualMotorBekas' dengan 2 kolom yang bernama 'judul' dan 'isi'. for i in range(len(bagan)): link = bagan[i].find( a )[ href ] page = requests.get(link) soup = BeautifulSoup(page.content, html.parser ) judul = soup.find(class_= single-listing ).getText() isi = soup.find(class_= single-main ) paragraf = isi.findAll( p ) p = for s in paragraf: p+=s.getText() + koneksi.execute( INSERT INTO jualMotorBekas values (?,?) , (judul, p)); Code diatas memiliki fungsi jika \"bagan = soup.findAll(class_='item-header')\" diklik akan menuju ke alamat setiap motor bekas yang dijual. mengambil data dari class 'single-listing' yang berisi judul/header motor yang dijual. dan akan mengambil data dari class 'single-main' yang berisi isi/deskripsi dari motor yang dijual. paragraf berfungsi untuk mengambil data sesuai dengan bentuk paragraf. dan akan diinsertkan/dimasukkan data yang di ambil ke dalam table jualMotorBekas berdasarkan kolom (judul,p). koneksi.commit() tampil = koneksi.execute( SELECT * FROM jualMotorBekas ) with open ( data_crawler.csv , newline= , mode= w )as employee_file : employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) \u200b``` for i in tampil: employee_writer.writerow(i) \u200b``` dari data yang telah diambil akan dimasukkan/ditulis kedalam bentuk csv dengan nama data_crawler.csv.","title":"penjelasan code crawling data :"},{"location":"crawllink/","text":"Crawl Link pengertian Web struncture mining dikenal juga sebagai web log mining adalah teknik yang digunakan untuk menemukan struktur link dari hyperlink dan membangun rangkuman website dan halaman web. Salah satu manfaatnya adlah untuk menentukan pagerank pada suatu halaman web. program import pandas as pd import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt penjelesan : mengimportkan librari diatas def simplifiedURL ( url ): # cek 1 : www if www. in url : ind = url . index ( www. ) + 4 url = http:// + url [ ind :] # cek 3 : tanda / di akhir if url [ - 1 ] == / : url = url [: - 1 ] # Cek 4 : cuma domain utama parts = url . split ( / ) url = for i in range ( 3 ): url += parts [ i ] + / return url penjelasan : mengambil link yang menjadi target dan mengecek link yang ada di crawl dengan memastikan tidak ada link yang sama serta menambahkan \"htpps\" pada link yang digunakan def crawl ( url , max_deep , show = False , deep = 0 , done = []): global edgelist deep += 1 url = simplifiedURL ( url ) if not url in done : links = getAllLinks ( url ) done . append ( url ) if show : if deep == 1 : print ( url ) else : print ( | , end = ) for i in range ( deep - 1 ): print ( -- , end = ) print ( ( %d ) %s % ( len ( links ), url )) for link in links : link = simplifiedURL ( link ) edge = ( url , link ) if not edge in edgelist : edgelist . append ( edge ) if ( deep != max_deep ): crawl ( link , max_deep , show , deep , done ) penjelasan : melakukan crawl dengan 4 parameter deep : mengecek kedalaman crawl show : menampilkan hasil crawl max_deep : batasan kedalaman saat proses crawl url : target alamat yang akan di crawling if not url in done : mengecrawl semua link yang ada for link in links : mengecek apakah jalan proses crawl sudah pernah di lalui, apabila belum maka akan dimasukkan kedalam list if (deep != max_deep) : mengecek kedalaman crawl def getAllLinks ( src ): page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) tags = soup . findAll ( a ) links = [] for tag in tags : try : link = tag [ href ] if not link in links and http in link : links . append ( link ) except KeyError : pass return links root = https://www.transtv.co.id/ nodelist = [ root ] edgelist = [] penjelasan : link yang menjadi target : https://www.transtv.co.id/ menuju ke link yang menjadi target mengubah html ke objek soup mengambil semua tag (a) root berfungsi untuk menyimpan data yang akan diproses dan membuat list dengan edgelist","title":"Crawl"},{"location":"crawllink/#crawl-link","text":"","title":"Crawl Link"},{"location":"crawllink/#pengertian","text":"Web struncture mining dikenal juga sebagai web log mining adalah teknik yang digunakan untuk menemukan struktur link dari hyperlink dan membangun rangkuman website dan halaman web. Salah satu manfaatnya adlah untuk menentukan pagerank pada suatu halaman web.","title":"pengertian"},{"location":"crawllink/#program","text":"import pandas as pd import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt penjelesan : mengimportkan librari diatas def simplifiedURL ( url ): # cek 1 : www if www. in url : ind = url . index ( www. ) + 4 url = http:// + url [ ind :] # cek 3 : tanda / di akhir if url [ - 1 ] == / : url = url [: - 1 ] # Cek 4 : cuma domain utama parts = url . split ( / ) url = for i in range ( 3 ): url += parts [ i ] + / return url penjelasan : mengambil link yang menjadi target dan mengecek link yang ada di crawl dengan memastikan tidak ada link yang sama serta menambahkan \"htpps\" pada link yang digunakan def crawl ( url , max_deep , show = False , deep = 0 , done = []): global edgelist deep += 1 url = simplifiedURL ( url ) if not url in done : links = getAllLinks ( url ) done . append ( url ) if show : if deep == 1 : print ( url ) else : print ( | , end = ) for i in range ( deep - 1 ): print ( -- , end = ) print ( ( %d ) %s % ( len ( links ), url )) for link in links : link = simplifiedURL ( link ) edge = ( url , link ) if not edge in edgelist : edgelist . append ( edge ) if ( deep != max_deep ): crawl ( link , max_deep , show , deep , done ) penjelasan : melakukan crawl dengan 4 parameter deep : mengecek kedalaman crawl show : menampilkan hasil crawl max_deep : batasan kedalaman saat proses crawl url : target alamat yang akan di crawling if not url in done : mengecrawl semua link yang ada for link in links : mengecek apakah jalan proses crawl sudah pernah di lalui, apabila belum maka akan dimasukkan kedalam list if (deep != max_deep) : mengecek kedalaman crawl def getAllLinks ( src ): page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) tags = soup . findAll ( a ) links = [] for tag in tags : try : link = tag [ href ] if not link in links and http in link : links . append ( link ) except KeyError : pass return links root = https://www.transtv.co.id/ nodelist = [ root ] edgelist = [] penjelasan : link yang menjadi target : https://www.transtv.co.id/ menuju ke link yang menjadi target mengubah html ke objek soup mengambil semua tag (a) root berfungsi untuk menyimpan data yang akan diproses dan membuat list dengan edgelist","title":"program"},{"location":"graph/","text":"Graph Graph adalah kumpulan dari titik ( node ) dan garis dimana pasangan-pasangan titik ( node ) tersebut dihubungkan oleh segmen garis. Node ini biasa disebut simpul (verteks) dan segmen garis disebut ruas (edge). program crawl ( root , 3 , show = True ) edgelistFrame = pd . DataFrame ( edgelist , None , ( From , To )) g = nx . from_pandas_edgelist ( edgelistFrame , From , To , None , nx . DiGraph ()) pos = nx . spring_layout ( g ) nx . draw ( g , pos ) nx . draw_networkx_labels ( g , pos , label , font_color = w ) plt . axis ( off ) plt . show () penjelasan : ketika proses crawl sudah dilakukan maka data yang tersimpan pada list akan diproses untuk dijadikan graph. pada proses graph ini menggunakan library network yang akan diproses menggunakan matplotlib.pyplot","title":"Graph"},{"location":"graph/#graph","text":"Graph adalah kumpulan dari titik ( node ) dan garis dimana pasangan-pasangan titik ( node ) tersebut dihubungkan oleh segmen garis. Node ini biasa disebut simpul (verteks) dan segmen garis disebut ruas (edge).","title":"Graph"},{"location":"graph/#program","text":"crawl ( root , 3 , show = True ) edgelistFrame = pd . DataFrame ( edgelist , None , ( From , To )) g = nx . from_pandas_edgelist ( edgelistFrame , From , To , None , nx . DiGraph ()) pos = nx . spring_layout ( g ) nx . draw ( g , pos ) nx . draw_networkx_labels ( g , pos , label , font_color = w ) plt . axis ( off ) plt . show () penjelasan : ketika proses crawl sudah dilakukan maka data yang tersimpan pada list akan diproses untuk dijadikan graph. pada proses graph ini menggunakan library network yang akan diproses menggunakan matplotlib.pyplot","title":"program"},{"location":"pagerank/","text":"Pagerank PageRank adalah sebuah algoritme yang telah dipatenkan yang berfungsi menentukan situs web mana yang lebih penting/populer. program damping = 0.85 max_iterr = 100 error_toleransi = 0.0001 pr = nx . pagerank ( g , alpha = damping , max_iter = max_iterr , tol = error_toleransi ) print ( keterangan node: ) nodelist = g . nodes label = {} data = [] for i , key in enumerate ( nodelist ): data . append (( pr [ key ], key )) label [ key ] = i urut = data . copy () for x in range ( len ( urut )): for y in range ( len ( urut )): if urut [ x ][ 0 ] urut [ y ][ 0 ]: urut [ x ], urut [ y ] = urut [ y ], urut [ x ] urut = pd . DataFrame ( urut , None , ( PageRank , Node )) print ( urut ) penjelasan : code diatas berfungsi untuk menentukan hasil pagerank dari tertinggi ke terkecil yang didapat dari proses crawl yang ditampung didalam list","title":"Pagerank"},{"location":"pagerank/#pagerank","text":"PageRank adalah sebuah algoritme yang telah dipatenkan yang berfungsi menentukan situs web mana yang lebih penting/populer.","title":"Pagerank"},{"location":"pagerank/#program","text":"damping = 0.85 max_iterr = 100 error_toleransi = 0.0001 pr = nx . pagerank ( g , alpha = damping , max_iter = max_iterr , tol = error_toleransi ) print ( keterangan node: ) nodelist = g . nodes label = {} data = [] for i , key in enumerate ( nodelist ): data . append (( pr [ key ], key )) label [ key ] = i urut = data . copy () for x in range ( len ( urut )): for y in range ( len ( urut )): if urut [ x ][ 0 ] urut [ y ][ 0 ]: urut [ x ], urut [ y ] = urut [ y ], urut [ x ] urut = pd . DataFrame ( urut , None , ( PageRank , Node )) print ( urut ) penjelasan : code diatas berfungsi untuk menentukan hasil pagerank dari tertinggi ke terkecil yang didapat dari proses crawl yang ditampung didalam list","title":"program"},{"location":"preprocessing/","text":"Proprocessing #vsm factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover () factory = StemmerFactory () stemmer = factory.create_stemmer () tmp = for i in isi: tmp = tmp + +i hasil = [] for i in tmp.split(): if i.isalpha() and not i in hasil: # Menghilangkan Kata tidak penting \u200b stop = stopword.remove(i) \u200b stem = stemmer.stem(stop) \u200b hasil.append((stem + )) print( menghilangkan kata tidak penting , hasil) katadasar = hasil #KBBI koneksi = sqlite3.connect( KBI.db ) cur_kbi = koneksi.execute( SELECT* from KATA ) def LinearSearch (kbi,kata): found=False posisi=0 while posisi len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) print( hasil KBI = ,berhasil) katadasar = np.array(berhasil) #(katadasar) matrix = [] for row in isi : tamp_isi=[] for a in katadasar: tamp_isi.append(row.lower().count(a)) matrix.append(tamp_isi) with open ( data_matrix.csv , newline= , mode= w )as employee_file : employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in matrix : employee_writer.writerow(i) #tf-idf df = list() for d in range (len(matrix[0])): total = 0 for i in range(len(matrix)): if matrix[i][d] !=0: total += 1 df.append(total) idf = list() for i in df: tmp = 1 + log10(len(matrix)/(1+i)) idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): tampungBaris = [] for kolom in range(len(matrix[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) with open( tf-idf.csv , newline= , mode= w ) as employee_file: employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in tfidf: employee_writer.writerow(i) Penjelasan Code Pre Processing : factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover () factory = StemmerFactory () stemmer = factory.create_stemmer () tmp = for i in isi: tmp = tmp + +i hasil = [] for i in tmp.split(): if i.isalpha() and not i in hasil: # Menghilangkan Kata tidak penting stop = stopword.remove(i) stem = stemmer.stem(stop) hasil.append((stem + )) print( menghilangkan kata tidak penting , hasil) katadasar = hasil PreProcessing yang pertama adalah Vector Space Model (VSM) pre processing pada VSM dengan cara menghitung setiap kata pada setiap dokumen. pada code (topWordRemoverFactory) ini juga akan menghapus kata yang tidak penting seperti \"dan\",\"di\",\"atau\". koneksi = sqlite3.connect( KBI.db ) cur_kbi = koneksi.execute( SELECT* from KATA ) def LinearSearch (kbi,kata): found=False posisi=0 while posisi len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) print( hasil KBI = ,berhasil) katadasar = np.array(berhasil) pada code ini digunakan untuk menseleksi kata yang tidak baku atau yang tidak ada dalam Kamus Besar Bahasa Indonesia (KBBI) dengan mengambil data kata yang ada di database KBI.db. matrix = [] for row in isi : tamp_isi=[] for a in katadasar: tamp_isi.append(row.lower().count(a)) matrix.append(tamp_isi) with open ( data_matrix.csv , newline= , mode= w )as employee_file : employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in matrix : employee_writer.writerow(i) pada code ini berfungsi untuk menghitung katadasar disetiap dkoumen/artikel dan akan menuliskannya pada file data_matrix.csv df = list() for d in range (len(matrix[0])): total = 0 for i in range(len(matrix)): if matrix[i][d] !=0: total += 1 df.append(total) idf = list() for i in df: tmp = 1 + log10(len(matrix)/(1+i)) idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): tampungBaris = [] for kolom in range(len(matrix[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) with open( tf-idf.csv , newline= , mode= w ) as employee_file: employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in tfidf: employee_writer.writerow(i) tf-idf merupakan perhitungan antara tf dan idf tf yang telah kita cari dengan cara menghitung katadasar dengan variabel 'matriks' yang bertujuan untuk menghitung berapa banyak kata disetiap dokumen/artiker sedangkan idf bertujuan untuk menghitung ada berapa kata tersebut disetiap dokumen/artikelnya. rumus tf-idf sendiri 'tmp = tf[baris][kolom] * idf[kolom]' dan hasilnya akan ditulis pada file tf-idf.csv","title":"Pre-Processing"},{"location":"preprocessing/#proprocessing","text":"#vsm factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover () factory = StemmerFactory () stemmer = factory.create_stemmer () tmp = for i in isi: tmp = tmp + +i hasil = [] for i in tmp.split(): if i.isalpha() and not i in hasil: # Menghilangkan Kata tidak penting \u200b stop = stopword.remove(i) \u200b stem = stemmer.stem(stop) \u200b hasil.append((stem + )) print( menghilangkan kata tidak penting , hasil) katadasar = hasil #KBBI koneksi = sqlite3.connect( KBI.db ) cur_kbi = koneksi.execute( SELECT* from KATA ) def LinearSearch (kbi,kata): found=False posisi=0 while posisi len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) print( hasil KBI = ,berhasil) katadasar = np.array(berhasil) #(katadasar) matrix = [] for row in isi : tamp_isi=[] for a in katadasar: tamp_isi.append(row.lower().count(a)) matrix.append(tamp_isi) with open ( data_matrix.csv , newline= , mode= w )as employee_file : employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in matrix : employee_writer.writerow(i) #tf-idf df = list() for d in range (len(matrix[0])): total = 0 for i in range(len(matrix)): if matrix[i][d] !=0: total += 1 df.append(total) idf = list() for i in df: tmp = 1 + log10(len(matrix)/(1+i)) idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): tampungBaris = [] for kolom in range(len(matrix[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) with open( tf-idf.csv , newline= , mode= w ) as employee_file: employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in tfidf: employee_writer.writerow(i)","title":"Proprocessing"},{"location":"preprocessing/#penjelasan-code-pre-processing","text":"factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover () factory = StemmerFactory () stemmer = factory.create_stemmer () tmp = for i in isi: tmp = tmp + +i hasil = [] for i in tmp.split(): if i.isalpha() and not i in hasil: # Menghilangkan Kata tidak penting stop = stopword.remove(i) stem = stemmer.stem(stop) hasil.append((stem + )) print( menghilangkan kata tidak penting , hasil) katadasar = hasil PreProcessing yang pertama adalah Vector Space Model (VSM) pre processing pada VSM dengan cara menghitung setiap kata pada setiap dokumen. pada code (topWordRemoverFactory) ini juga akan menghapus kata yang tidak penting seperti \"dan\",\"di\",\"atau\". koneksi = sqlite3.connect( KBI.db ) cur_kbi = koneksi.execute( SELECT* from KATA ) def LinearSearch (kbi,kata): found=False posisi=0 while posisi len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) print( hasil KBI = ,berhasil) katadasar = np.array(berhasil) pada code ini digunakan untuk menseleksi kata yang tidak baku atau yang tidak ada dalam Kamus Besar Bahasa Indonesia (KBBI) dengan mengambil data kata yang ada di database KBI.db. matrix = [] for row in isi : tamp_isi=[] for a in katadasar: tamp_isi.append(row.lower().count(a)) matrix.append(tamp_isi) with open ( data_matrix.csv , newline= , mode= w )as employee_file : employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in matrix : employee_writer.writerow(i) pada code ini berfungsi untuk menghitung katadasar disetiap dkoumen/artikel dan akan menuliskannya pada file data_matrix.csv df = list() for d in range (len(matrix[0])): total = 0 for i in range(len(matrix)): if matrix[i][d] !=0: total += 1 df.append(total) idf = list() for i in df: tmp = 1 + log10(len(matrix)/(1+i)) idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): tampungBaris = [] for kolom in range(len(matrix[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) with open( tf-idf.csv , newline= , mode= w ) as employee_file: employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in tfidf: employee_writer.writerow(i) tf-idf merupakan perhitungan antara tf dan idf tf yang telah kita cari dengan cara menghitung katadasar dengan variabel 'matriks' yang bertujuan untuk menghitung berapa banyak kata disetiap dokumen/artiker sedangkan idf bertujuan untuk menghitung ada berapa kata tersebut disetiap dokumen/artikelnya. rumus tf-idf sendiri 'tmp = tf[baris][kolom] * idf[kolom]' dan hasilnya akan ditulis pada file tf-idf.csv","title":"Penjelasan Code Pre Processing :"},{"location":"seleksiFitur/","text":"Seleksi Fitur def pearsonCalculate(data, u,v): i, j is an index atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] katadasarBaru=katadasar[:u+1] v = u while v len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar = katadasarBaru if u%50 == 0 : print( seleksi fitur = , u, data.shape) u+=1 return katadasar, data katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf, 0.8) Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. dan metode yang digunakan pada code diatas adalah Pearson Correlation. dimana Pearson Correlation setiap fitur akan dihitung korelasinya. Semakin tinggi nilainya, maka fitur tersebut semakin kuat korelasinya. Lalu fitur yang memiliki korelasi tinggi akan dibuang salah satunya.","title":"Seleksi Fitur"}]}